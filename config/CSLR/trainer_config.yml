trainer_cslr:
  cwd: /home/papastrat/PycharmProjects/PyramidTransformer # working directory
  logger: CSLR # logger name
  epochs: 30 # number of training epochs
  seed: 1234 # randomness seed
  cuda: True  # use nvidia gpu
  gpu: 1 # id of gpu
  save: True # swave checkpoint
  load: True # load pretrained checkpoint
  gradient_accumulation: 2 # gradient accumulation steps
  pretrained_cpkt: /home/papastrat/Desktop/ilias/SLVTP_PL/checkpoints/date_22_07_2021_18.32.39/epoch=4-step=10939.ckpt
  log_interval: 1000 # print statistics every log_interval
  model:
    name: CSLR_2dcnntcl # model name
    optimizer: # optimizer configuration
      type: Adam # optimizer type
      lr: 1e-4 # learning rate
      weight_decay: 0.000001 # weight decay
    scheduler: # learning rate scheduler
      type: ReduceLRonPlateau # type of scheduler
      scheduler_factor: 0.8 # learning rate change ratio
      scheduler_patience: 2 # patience for some epochs
      scheduler_min_lr: 1e-5 # minimum learning rate value
      scheduler_verbose: 5e-6 # print if learning rate is changed
  dataloader:
    train:
      batch_size: 1 # batch size
      shuffle: True # shuffle samples after every epoch
      num_workers: 2 # number of thread for dataloader1
    val:
      batch_size: 1
      shuffle: False
      num_workers: 2
    test:
      batch_size: 1
      shuffle: False
      num_workers: 2
  dataset:
    input_data: /home/papastrat/Desktop/ilias/datasets/
    name: GSL_SI2 # dataset name
    modality: RGB # type of modality
    classes: 311 # number of classes
    normalize: True # mean, std normalization
    padding : False # pad videos
    dim: [224,224] # frame dimension
    train:
      seq_length: 250 # number of frames for each video
      augmentation: True # do augmentation to video
    val:
      seq_length: 250
      augmentation: False
    test:
      seq_length: 250
      augmentation: False